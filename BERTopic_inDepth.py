# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10BIR7qA-sNnA24Ru6zxwsqfZ2KwzR6tp
"""

pip install bertopic

# Choose an embedding backend
!pip install bertopic[flair, gensim, spacy, use]

# Topic modeling with images
!pip install bertopic[vision]

from bertopic import BERTopic
from sklearn.datasets import fetch_20newsgroups

docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data']

# Set calculate_probabilities to False
topic_model = BERTopic(calculate_probabilities=False)

topics, _ = topic_model.fit_transform(docs)

topic_model.get_topic_info()

topic_model.get_topic(0)

topic_model.visualize_topics()

embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
topic_model.save("path/to/my/model_dir", serialization="safetensors", save_ctfidf=True, save_embedding_model=embedding_model)

similar_topics, similarity = topic_model.find_topics("motor", top_n=5)

topic_model.get_topic(similar_topics[0])

similarity[0]

topic_model.get_topic(similar_topics[1])

similarity[1]

topic_model.get_topic(similar_topics[2])

similarity[2]

topic_model.get_topic(similar_topics[3])

topic_model.get_topic(1, full=True)

topic_model.get_topic_info()

# `topic_distr` contains the distribution of topics in each document
topic_distr, _ = topic_model.approximate_distribution(docs, window=8, stride=4)

abstract_id = 10
print(docs[0])

topic_model.visualize_distribution(topic_distr[0])

topic_distr, topic_token_distr = topic_model.approximate_distribution(docs[0], calculate_tokens=True)

# Visualize the token-level distributions
df = topic_model.visualize_approximate_distribution(docs[0], topic_token_distr[0])
df

topic_distr, _ = topic_model.approximate_distribution(docs, use_embedding_model=True)

topic_model.visualize_topics(custom_labels=True)

topic_model.visualize_hierarchy(custom_labels=True)

from sentence_transformers import SentenceTransformer

# Pre-calculate embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedding_model.encode(docs, show_progress_bar=True)

from umap import UMAP

umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)

reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)

topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, custom_labels=True)

topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, custom_labels=True, hide_annotations=True)

embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
topic_model.save("my_model_dir", serialization="safetensors", save_ctfidf=True, save_embedding_model=embedding_model)

from sentence_transformers import SentenceTransformer

# Define embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Load model and add embedding model
loaded_model = BERTopic.load("path/to/my/model_dir", embedding_model=embedding_model)

from bertopic._utils import MyLogger
logger = MyLogger("ERROR")
loaded_model.verbose = False
topic_model.verbose = False

# Commented out IPython magic to ensure Python compatibility.
# %timeit loaded_model.transform(docs[:100])

# Commented out IPython magic to ensure Python compatibility.
# %timeit topic_model.transform(docs[:100])

